{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--test_flop'], dest='test_flop', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='See utils/tools for usage', metavar=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting', allow_abbrev=False)\n",
    "\n",
    "# random seed\n",
    "parser.add_argument('--random_seed', type=int, default=2021, help='random seed')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "#parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# PatchTST\n",
    "parser.add_argument('--fc_dropout', type=float, default=0.05, help='fully connected dropout')\n",
    "parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')\n",
    "parser.add_argument('--patch_len', type=int, default=16, help='patch length')\n",
    "parser.add_argument('--stride', type=int, default=8, help='stride')\n",
    "parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')\n",
    "parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')\n",
    "parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')\n",
    "parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')\n",
    "parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')\n",
    "parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')\n",
    "parser.add_argument('--individual', type=int, default=0, help='individual head; True 1 False 0')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=100, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=100, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type3', help='adjust learning rate')\n",
    "parser.add_argument('--pct_start', type=float, default=0.3, help='pct_start')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(f\"--is_training 1 --model_id roc_test_{np.random.randint(1, 999999999999999999)} --model PatchTST --data custom --data_path weather.csv --root_path ./dataset/ --itr 1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(random_seed=2021, is_training=1, model_id='roc_test_917326356193919207', model='PatchTST', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, fc_dropout=0.05, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=128, patience=100, learning_rate=0.0001, des='test', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n"
     ]
    }
   ],
   "source": [
    "fix_seed = args.random_seed\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.dvices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 36696\n"
     ]
    }
   ],
   "source": [
    "_, data_loader = Exp(args)._get_data(flag='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 48, 96, 'M', 'OT', 'h')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.seq_len, args.label_len, args.pred_len, args.features, args.target, args.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(data_loader):\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([128, 96, 21]),\n",
       " torch.Size([128, 144, 21]),\n",
       " torch.Size([128, 96, 4]),\n",
       " torch.Size([128, 144, 4])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in [batch_x, batch_y, batch_x_mark, batch_y_mark]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "144/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3696,  0.1667, -0.4333, -0.4945], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y_mark[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6717, -1.2086, -1.2404, -0.7834,  0.9368, -1.0216, -0.8085, -0.8029,\n",
       "        -0.8128, -0.8131,  1.2813,  0.0667,  1.3672,  0.2187, -0.0935, -0.2211,\n",
       "        -0.6728, -0.6795, -0.5883, -1.3690,  0.0452], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 96, 21])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 48, 21])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(batch_y[:, args.pred_len:, :]).float().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : roc_test_802032661458808150_PatchTST_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 36696\n",
      "val 5175\n",
      "test 10444\n",
      "\titers: 100, epoch: 1 | loss: 0.6793757\n",
      "\tspeed: 0.2050s/iter; left time: 5842.2774s\n",
      "\titers: 200, epoch: 1 | loss: 0.4189709\n",
      "\tspeed: 0.1951s/iter; left time: 5540.0690s\n",
      "Epoch: 1 cost time: 56.2715699672699\n",
      "Epoch: 1, Steps: 286 | Train Loss: 0.6363000 Vali Loss: 0.5150962 Test Loss: 0.2086973\n",
      "Validation loss decreased (inf --> 0.515096).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4167281\n",
      "\tspeed: 0.4597s/iter; left time: 12970.3546s\n",
      "\titers: 200, epoch: 2 | loss: 0.5208881\n",
      "\tspeed: 0.1950s/iter; left time: 5481.7564s\n",
      "Epoch: 2 cost time: 55.76416993141174\n",
      "Epoch: 2, Steps: 286 | Train Loss: 0.4877896 Vali Loss: 0.4498956 Test Loss: 0.1852286\n",
      "Validation loss decreased (0.515096 --> 0.449896).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 3 | loss: 0.3518353\n",
      "\tspeed: 0.4611s/iter; left time: 12879.0293s\n",
      "\titers: 200, epoch: 3 | loss: 0.3274820\n",
      "\tspeed: 0.1958s/iter; left time: 5448.1177s\n",
      "Epoch: 3 cost time: 56.02473449707031\n",
      "Epoch: 3, Steps: 286 | Train Loss: 0.4678302 Vali Loss: 0.4524038 Test Loss: 0.1860378\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 4 | loss: 0.3760040\n",
      "\tspeed: 0.4629s/iter; left time: 12795.6911s\n",
      "\titers: 200, epoch: 4 | loss: 0.6368050\n",
      "\tspeed: 0.1958s/iter; left time: 5394.0401s\n",
      "Epoch: 4 cost time: 56.106945514678955\n",
      "Epoch: 4, Steps: 286 | Train Loss: 0.4596650 Vali Loss: 0.4479505 Test Loss: 0.1825662\n",
      "Validation loss decreased (0.449896 --> 0.447950).  Saving model ...\n",
      "Updating learning rate to 9e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3835020\n",
      "\tspeed: 0.4631s/iter; left time: 12668.9872s\n",
      "\titers: 200, epoch: 5 | loss: 0.4893148\n",
      "\tspeed: 0.1964s/iter; left time: 5352.6886s\n",
      "Epoch: 5 cost time: 56.125664710998535\n",
      "Epoch: 5, Steps: 286 | Train Loss: 0.4539945 Vali Loss: 0.4444125 Test Loss: 0.1745831\n",
      "Validation loss decreased (0.447950 --> 0.444412).  Saving model ...\n",
      "Updating learning rate to 8.1e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3603590\n",
      "\tspeed: 0.4621s/iter; left time: 12508.9082s\n",
      "\titers: 200, epoch: 6 | loss: 0.5294219\n",
      "\tspeed: 0.1953s/iter; left time: 5267.1564s\n",
      "Epoch: 6 cost time: 55.92443823814392\n",
      "Epoch: 6, Steps: 286 | Train Loss: 0.4503070 Vali Loss: 0.4511243 Test Loss: 0.1824908\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 7.290000000000001e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.5905260\n",
      "\tspeed: 0.4634s/iter; left time: 12412.7836s\n",
      "\titers: 200, epoch: 7 | loss: 0.6983048\n",
      "\tspeed: 0.1974s/iter; left time: 5266.9310s\n",
      "Epoch: 7 cost time: 56.3280086517334\n",
      "Epoch: 7, Steps: 286 | Train Loss: 0.4486101 Vali Loss: 0.4427099 Test Loss: 0.1783302\n",
      "Validation loss decreased (0.444412 --> 0.442710).  Saving model ...\n",
      "Updating learning rate to 6.561e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.4305879\n",
      "\tspeed: 0.4633s/iter; left time: 12276.1331s\n",
      "\titers: 200, epoch: 8 | loss: 0.3577030\n",
      "\tspeed: 0.1961s/iter; left time: 5176.0514s\n",
      "Epoch: 8 cost time: 56.05690670013428\n",
      "Epoch: 8, Steps: 286 | Train Loss: 0.4444494 Vali Loss: 0.4373487 Test Loss: 0.1761299\n",
      "Validation loss decreased (0.442710 --> 0.437349).  Saving model ...\n",
      "Updating learning rate to 5.904900000000001e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.4043286\n",
      "\tspeed: 0.4627s/iter; left time: 12127.7559s\n",
      "\titers: 200, epoch: 9 | loss: 0.3055070\n",
      "\tspeed: 0.1951s/iter; left time: 5095.7145s\n",
      "Epoch: 9 cost time: 55.89500141143799\n",
      "Epoch: 9, Steps: 286 | Train Loss: 0.4413652 Vali Loss: 0.4431031 Test Loss: 0.1736991\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 5.3144100000000005e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.8731871\n",
      "\tspeed: 0.4615s/iter; left time: 11964.6841s\n",
      "\titers: 200, epoch: 10 | loss: 0.5905772\n",
      "\tspeed: 0.1954s/iter; left time: 5047.7218s\n",
      "Epoch: 10 cost time: 55.88002109527588\n",
      "Epoch: 10, Steps: 286 | Train Loss: 0.4399134 Vali Loss: 0.4482956 Test Loss: 0.1818184\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 4.782969000000001e-05\n",
      "\titers: 100, epoch: 11 | loss: 0.3876171\n",
      "\tspeed: 0.4618s/iter; left time: 11841.6502s\n",
      "\titers: 200, epoch: 11 | loss: 0.3317600\n",
      "\tspeed: 0.1956s/iter; left time: 4996.7868s\n",
      "Epoch: 11 cost time: 56.04958724975586\n",
      "Epoch: 11, Steps: 286 | Train Loss: 0.4394291 Vali Loss: 0.4400116 Test Loss: 0.1762588\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 4.304672100000001e-05\n",
      "\titers: 100, epoch: 12 | loss: 0.3084341\n",
      "\tspeed: 0.4627s/iter; left time: 11731.4061s\n",
      "\titers: 200, epoch: 12 | loss: 0.3316582\n",
      "\tspeed: 0.1959s/iter; left time: 4946.6570s\n",
      "Epoch: 12 cost time: 55.95991086959839\n",
      "Epoch: 12, Steps: 286 | Train Loss: 0.4359688 Vali Loss: 0.4343898 Test Loss: 0.1769289\n",
      "Validation loss decreased (0.437349 --> 0.434390).  Saving model ...\n",
      "Updating learning rate to 3.874204890000001e-05\n",
      "\titers: 100, epoch: 13 | loss: 0.4147369\n",
      "\tspeed: 0.4637s/iter; left time: 11623.6305s\n",
      "\titers: 200, epoch: 13 | loss: 0.3636273\n",
      "\tspeed: 0.1968s/iter; left time: 4914.1423s\n",
      "Epoch: 13 cost time: 56.34964323043823\n",
      "Epoch: 13, Steps: 286 | Train Loss: 0.4343876 Vali Loss: 0.4335847 Test Loss: 0.1763427\n",
      "Validation loss decreased (0.434390 --> 0.433585).  Saving model ...\n",
      "Updating learning rate to 3.486784401000001e-05\n",
      "\titers: 100, epoch: 14 | loss: 0.3252999\n",
      "\tspeed: 0.4642s/iter; left time: 11503.6457s\n",
      "\titers: 200, epoch: 14 | loss: 0.5578743\n",
      "\tspeed: 0.1973s/iter; left time: 4871.1832s\n",
      "Epoch: 14 cost time: 56.29794430732727\n",
      "Epoch: 14, Steps: 286 | Train Loss: 0.4322215 Vali Loss: 0.4337366 Test Loss: 0.1742287\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 3.138105960900001e-05\n",
      "\titers: 100, epoch: 15 | loss: 0.3537393\n",
      "\tspeed: 0.4647s/iter; left time: 11383.1209s\n",
      "\titers: 200, epoch: 15 | loss: 0.3713155\n",
      "\tspeed: 0.1962s/iter; left time: 4787.3252s\n",
      "Epoch: 15 cost time: 56.22527527809143\n",
      "Epoch: 15, Steps: 286 | Train Loss: 0.4311470 Vali Loss: 0.4383016 Test Loss: 0.1741888\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 2.824295364810001e-05\n",
      "\titers: 100, epoch: 16 | loss: 0.7191133\n",
      "\tspeed: 0.4639s/iter; left time: 11232.6531s\n",
      "\titers: 200, epoch: 16 | loss: 0.5422868\n",
      "\tspeed: 0.1966s/iter; left time: 4741.0356s\n",
      "Epoch: 16 cost time: 56.26432943344116\n",
      "Epoch: 16, Steps: 286 | Train Loss: 0.4300528 Vali Loss: 0.4406350 Test Loss: 0.1750008\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 2.541865828329001e-05\n",
      "\titers: 100, epoch: 17 | loss: 0.5724415\n",
      "\tspeed: 0.4639s/iter; left time: 11099.8734s\n",
      "\titers: 200, epoch: 17 | loss: 0.3511973\n",
      "\tspeed: 0.1966s/iter; left time: 4684.4395s\n",
      "Epoch: 17 cost time: 56.270212173461914\n",
      "Epoch: 17, Steps: 286 | Train Loss: 0.4292545 Vali Loss: 0.4319393 Test Loss: 0.1749980\n",
      "Validation loss decreased (0.433585 --> 0.431939).  Saving model ...\n",
      "Updating learning rate to 2.287679245496101e-05\n",
      "\titers: 100, epoch: 18 | loss: 0.3393344\n",
      "\tspeed: 0.4648s/iter; left time: 10986.9595s\n",
      "\titers: 200, epoch: 18 | loss: 0.5166203\n",
      "\tspeed: 0.1972s/iter; left time: 4642.3708s\n",
      "Epoch: 18 cost time: 56.331796407699585\n",
      "Epoch: 18, Steps: 286 | Train Loss: 0.4288195 Vali Loss: 0.4358466 Test Loss: 0.1785678\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 2.0589113209464907e-05\n",
      "\titers: 100, epoch: 19 | loss: 0.3135601\n",
      "\tspeed: 0.4643s/iter; left time: 10841.9237s\n",
      "\titers: 200, epoch: 19 | loss: 0.3282595\n",
      "\tspeed: 0.1975s/iter; left time: 4592.6784s\n",
      "Epoch: 19 cost time: 56.353015661239624\n",
      "Epoch: 19, Steps: 286 | Train Loss: 0.4270617 Vali Loss: 0.4322514 Test Loss: 0.1747280\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 1.8530201888518416e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.5662103\n",
      "\tspeed: 0.4645s/iter; left time: 10714.9893s\n",
      "\titers: 200, epoch: 20 | loss: 0.3903566\n",
      "\tspeed: 0.1966s/iter; left time: 4516.3100s\n",
      "Epoch: 20 cost time: 56.20507216453552\n",
      "Epoch: 20, Steps: 286 | Train Loss: 0.4263621 Vali Loss: 0.4326530 Test Loss: 0.1754974\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 1.6677181699666577e-05\n",
      "\titers: 100, epoch: 21 | loss: 0.3395317\n",
      "\tspeed: 0.4628s/iter; left time: 10544.1546s\n",
      "\titers: 200, epoch: 21 | loss: 0.3386176\n",
      "\tspeed: 0.1960s/iter; left time: 4445.1652s\n",
      "Epoch: 21 cost time: 56.08058166503906\n",
      "Epoch: 21, Steps: 286 | Train Loss: 0.4257682 Vali Loss: 0.4302764 Test Loss: 0.1751244\n",
      "Validation loss decreased (0.431939 --> 0.430276).  Saving model ...\n",
      "Updating learning rate to 1.5009463529699919e-05\n",
      "\titers: 100, epoch: 22 | loss: 0.9157191\n",
      "\tspeed: 0.4638s/iter; left time: 10434.0344s\n",
      "\titers: 200, epoch: 22 | loss: 0.3338754\n",
      "\tspeed: 0.1970s/iter; left time: 4410.9746s\n",
      "Epoch: 22 cost time: 56.35046935081482\n",
      "Epoch: 22, Steps: 286 | Train Loss: 0.4251584 Vali Loss: 0.4351059 Test Loss: 0.1734000\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 1.3508517176729929e-05\n",
      "\titers: 100, epoch: 23 | loss: 0.4041091\n",
      "\tspeed: 0.4647s/iter; left time: 10320.7577s\n",
      "\titers: 200, epoch: 23 | loss: 0.6482869\n",
      "\tspeed: 0.1970s/iter; left time: 4356.3978s\n",
      "Epoch: 23 cost time: 56.35010528564453\n",
      "Epoch: 23, Steps: 286 | Train Loss: 0.4247625 Vali Loss: 0.4346332 Test Loss: 0.1763309\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 1.2157665459056936e-05\n",
      "\titers: 100, epoch: 24 | loss: 0.2889386\n",
      "\tspeed: 0.4648s/iter; left time: 10188.9796s\n",
      "\titers: 200, epoch: 24 | loss: 0.3345507\n",
      "\tspeed: 0.1966s/iter; left time: 4290.9839s\n",
      "Epoch: 24 cost time: 56.28359794616699\n",
      "Epoch: 24, Steps: 286 | Train Loss: 0.4243039 Vali Loss: 0.4327589 Test Loss: 0.1751621\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 1.0941898913151242e-05\n",
      "\titers: 100, epoch: 25 | loss: 0.3159894\n",
      "\tspeed: 0.4642s/iter; left time: 10044.5388s\n",
      "\titers: 200, epoch: 25 | loss: 0.3335754\n",
      "\tspeed: 0.1965s/iter; left time: 4231.0016s\n",
      "Epoch: 25 cost time: 56.16817808151245\n",
      "Epoch: 25, Steps: 286 | Train Loss: 0.4236163 Vali Loss: 0.4344896 Test Loss: 0.1743633\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 9.847709021836118e-06\n",
      "\titers: 100, epoch: 26 | loss: 0.3015578\n",
      "\tspeed: 0.4633s/iter; left time: 9892.8919s\n",
      "\titers: 200, epoch: 26 | loss: 0.5265115\n",
      "\tspeed: 0.1959s/iter; left time: 4163.9080s\n",
      "Epoch: 26 cost time: 56.134310245513916\n",
      "Epoch: 26, Steps: 286 | Train Loss: 0.4224064 Vali Loss: 0.4373854 Test Loss: 0.1753593\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 8.862938119652508e-06\n",
      "\titers: 100, epoch: 27 | loss: 0.3947566\n",
      "\tspeed: 0.4632s/iter; left time: 9757.7657s\n",
      "\titers: 200, epoch: 27 | loss: 0.3491571\n",
      "\tspeed: 0.1964s/iter; left time: 4116.8447s\n",
      "Epoch: 27 cost time: 56.14741635322571\n",
      "Epoch: 27, Steps: 286 | Train Loss: 0.4226494 Vali Loss: 0.4366903 Test Loss: 0.1748749\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 7.976644307687255e-06\n",
      "\titers: 100, epoch: 28 | loss: 0.5003740\n",
      "\tspeed: 0.4625s/iter; left time: 9610.3630s\n",
      "\titers: 200, epoch: 28 | loss: 0.3077132\n",
      "\tspeed: 0.1965s/iter; left time: 4064.0329s\n",
      "Epoch: 28 cost time: 56.21561598777771\n",
      "Epoch: 28, Steps: 286 | Train Loss: 0.4223409 Vali Loss: 0.4374648 Test Loss: 0.1767930\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 7.178979876918531e-06\n",
      "\titers: 100, epoch: 29 | loss: 0.4102197\n",
      "\tspeed: 0.4645s/iter; left time: 9518.1487s\n",
      "\titers: 200, epoch: 29 | loss: 0.3748322\n",
      "\tspeed: 0.1965s/iter; left time: 4007.1784s\n",
      "Epoch: 29 cost time: 56.23395252227783\n",
      "Epoch: 29, Steps: 286 | Train Loss: 0.4219192 Vali Loss: 0.4360230 Test Loss: 0.1755316\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 6.4610818892266776e-06\n",
      "\titers: 100, epoch: 30 | loss: 0.4601036\n",
      "\tspeed: 0.4630s/iter; left time: 9356.6610s\n",
      "\titers: 200, epoch: 30 | loss: 0.3471140\n",
      "\tspeed: 0.1968s/iter; left time: 3956.7870s\n",
      "Epoch: 30 cost time: 56.21656608581543\n",
      "Epoch: 30, Steps: 286 | Train Loss: 0.4216030 Vali Loss: 0.4347251 Test Loss: 0.1753126\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 5.8149737003040096e-06\n",
      "\titers: 100, epoch: 31 | loss: 0.3156099\n",
      "\tspeed: 0.4633s/iter; left time: 9229.7182s\n",
      "\titers: 200, epoch: 31 | loss: 0.5571315\n",
      "\tspeed: 0.1962s/iter; left time: 3887.9937s\n",
      "Epoch: 31 cost time: 56.183292388916016\n",
      "Epoch: 31, Steps: 286 | Train Loss: 0.4213217 Vali Loss: 0.4396585 Test Loss: 0.1743587\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 5.23347633027361e-06\n",
      "\titers: 100, epoch: 32 | loss: 0.3084736\n",
      "\tspeed: 0.4635s/iter; left time: 9100.9474s\n",
      "\titers: 200, epoch: 32 | loss: 0.3779780\n",
      "\tspeed: 0.1967s/iter; left time: 3841.6622s\n",
      "Epoch: 32 cost time: 56.28769111633301\n",
      "Epoch: 32, Steps: 286 | Train Loss: 0.4209145 Vali Loss: 0.4403537 Test Loss: 0.1755458\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 4.710128697246249e-06\n",
      "\titers: 100, epoch: 33 | loss: 0.3104341\n",
      "\tspeed: 0.4652s/iter; left time: 9001.8243s\n",
      "\titers: 200, epoch: 33 | loss: 0.3981098\n",
      "\tspeed: 0.1964s/iter; left time: 3781.0274s\n",
      "Epoch: 33 cost time: 56.24018669128418\n",
      "Epoch: 33, Steps: 286 | Train Loss: 0.4190800 Vali Loss: 0.4357120 Test Loss: 0.1748708\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Updating learning rate to 4.239115827521624e-06\n",
      "\titers: 100, epoch: 34 | loss: 0.4054053\n",
      "\tspeed: 0.4632s/iter; left time: 8830.8714s\n",
      "\titers: 200, epoch: 34 | loss: 0.3770504\n",
      "\tspeed: 0.1965s/iter; left time: 3726.2016s\n",
      "Epoch: 34 cost time: 56.16125178337097\n",
      "Epoch: 34, Steps: 286 | Train Loss: 0.4205561 Vali Loss: 0.4373199 Test Loss: 0.1745444\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Updating learning rate to 3.815204244769462e-06\n",
      "\titers: 100, epoch: 35 | loss: 0.5992493\n",
      "\tspeed: 0.4634s/iter; left time: 8702.0555s\n",
      "\titers: 200, epoch: 35 | loss: 0.8074284\n",
      "\tspeed: 0.1969s/iter; left time: 3678.2844s\n",
      "Epoch: 35 cost time: 56.30480623245239\n",
      "Epoch: 35, Steps: 286 | Train Loss: 0.4194938 Vali Loss: 0.4322416 Test Loss: 0.1740406\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Updating learning rate to 3.4336838202925152e-06\n",
      "\titers: 100, epoch: 36 | loss: 0.2880275\n",
      "\tspeed: 0.4637s/iter; left time: 8574.6522s\n",
      "\titers: 200, epoch: 36 | loss: 0.4016696\n",
      "\tspeed: 0.1964s/iter; left time: 3612.3222s\n",
      "Epoch: 36 cost time: 56.14674353599548\n",
      "Epoch: 36, Steps: 286 | Train Loss: 0.4198890 Vali Loss: 0.4371881 Test Loss: 0.1743499\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Updating learning rate to 3.090315438263264e-06\n",
      "\titers: 100, epoch: 37 | loss: 0.5007514\n",
      "\tspeed: 0.4639s/iter; left time: 8445.9954s\n",
      "\titers: 200, epoch: 37 | loss: 0.3512994\n",
      "\tspeed: 0.1965s/iter; left time: 3556.9953s\n",
      "Epoch: 37 cost time: 56.26381278038025\n",
      "Epoch: 37, Steps: 286 | Train Loss: 0.4192421 Vali Loss: 0.4390903 Test Loss: 0.1749027\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Updating learning rate to 2.7812838944369375e-06\n",
      "\titers: 100, epoch: 38 | loss: 0.3153769\n",
      "\tspeed: 0.4631s/iter; left time: 8297.5873s\n",
      "\titers: 200, epoch: 38 | loss: 0.2981620\n",
      "\tspeed: 0.1966s/iter; left time: 3503.1506s\n",
      "Epoch: 38 cost time: 56.21184158325195\n",
      "Epoch: 38, Steps: 286 | Train Loss: 0.4199342 Vali Loss: 0.4344853 Test Loss: 0.1740529\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Updating learning rate to 2.503155504993244e-06\n",
      "\titers: 100, epoch: 39 | loss: 0.3182374\n",
      "\tspeed: 0.4628s/iter; left time: 8160.6902s\n",
      "\titers: 200, epoch: 39 | loss: 0.3015098\n",
      "\tspeed: 0.1965s/iter; left time: 3444.8322s\n",
      "Epoch: 39 cost time: 56.15597414970398\n",
      "Epoch: 39, Steps: 286 | Train Loss: 0.4196309 Vali Loss: 0.4373477 Test Loss: 0.1746673\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Updating learning rate to 2.2528399544939195e-06\n",
      "\titers: 100, epoch: 40 | loss: 0.4311716\n",
      "\tspeed: 0.4642s/iter; left time: 8052.7350s\n",
      "\titers: 200, epoch: 40 | loss: 0.3326641\n",
      "\tspeed: 0.1966s/iter; left time: 3390.8797s\n",
      "Epoch: 40 cost time: 56.195196866989136\n",
      "Epoch: 40, Steps: 286 | Train Loss: 0.4193199 Vali Loss: 0.4374906 Test Loss: 0.1746946\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Updating learning rate to 2.0275559590445276e-06\n",
      "\titers: 100, epoch: 41 | loss: 0.3900795\n",
      "\tspeed: 0.4642s/iter; left time: 7919.6178s\n",
      "\titers: 200, epoch: 41 | loss: 0.3320616\n",
      "\tspeed: 0.1961s/iter; left time: 3325.7263s\n",
      "Epoch: 41 cost time: 56.20522141456604\n",
      "Epoch: 41, Steps: 286 | Train Loss: 0.4193176 Vali Loss: 0.4363407 Test Loss: 0.1748669\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Updating learning rate to 1.8248003631400751e-06\n",
      "\titers: 100, epoch: 42 | loss: 0.4040139\n",
      "\tspeed: 0.4639s/iter; left time: 7781.3212s\n",
      "\titers: 200, epoch: 42 | loss: 0.3834336\n",
      "\tspeed: 0.1965s/iter; left time: 3275.9460s\n",
      "Epoch: 42 cost time: 56.27082300186157\n",
      "Epoch: 42, Steps: 286 | Train Loss: 0.4189046 Vali Loss: 0.4364576 Test Loss: 0.1746884\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Updating learning rate to 1.6423203268260676e-06\n",
      "\titers: 100, epoch: 43 | loss: 0.3585700\n",
      "\tspeed: 0.4634s/iter; left time: 7641.6116s\n",
      "\titers: 200, epoch: 43 | loss: 0.3391272\n",
      "\tspeed: 0.1964s/iter; left time: 3219.0063s\n",
      "Epoch: 43 cost time: 56.09638071060181\n",
      "Epoch: 43, Steps: 286 | Train Loss: 0.4191941 Vali Loss: 0.4365669 Test Loss: 0.1740845\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Updating learning rate to 1.4780882941434609e-06\n",
      "\titers: 100, epoch: 44 | loss: 0.5987700\n",
      "\tspeed: 0.4637s/iter; left time: 7512.7544s\n",
      "\titers: 200, epoch: 44 | loss: 0.3619180\n",
      "\tspeed: 0.1966s/iter; left time: 3165.4885s\n",
      "Epoch: 44 cost time: 56.25788855552673\n",
      "Epoch: 44, Steps: 286 | Train Loss: 0.4181971 Vali Loss: 0.4377015 Test Loss: 0.1746576\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Updating learning rate to 1.3302794647291146e-06\n",
      "\titers: 100, epoch: 45 | loss: 0.4170266\n",
      "\tspeed: 0.4643s/iter; left time: 7390.7655s\n",
      "\titers: 200, epoch: 45 | loss: 0.3641686\n",
      "\tspeed: 0.1969s/iter; left time: 3114.0985s\n",
      "Epoch: 45 cost time: 56.24271869659424\n",
      "Epoch: 45, Steps: 286 | Train Loss: 0.4192174 Vali Loss: 0.4366403 Test Loss: 0.1748060\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Updating learning rate to 1.1972515182562034e-06\n",
      "\titers: 100, epoch: 46 | loss: 0.3173723\n",
      "\tspeed: 0.4639s/iter; left time: 7251.2775s\n",
      "\titers: 200, epoch: 46 | loss: 0.2601127\n",
      "\tspeed: 0.1964s/iter; left time: 3049.8011s\n",
      "Epoch: 46 cost time: 56.239142417907715\n",
      "Epoch: 46, Steps: 286 | Train Loss: 0.4190996 Vali Loss: 0.4361351 Test Loss: 0.1743269\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Updating learning rate to 1.077526366430583e-06\n",
      "\titers: 100, epoch: 47 | loss: 0.6087328\n",
      "\tspeed: 0.4642s/iter; left time: 7123.0134s\n",
      "\titers: 200, epoch: 47 | loss: 0.3004319\n",
      "\tspeed: 0.1966s/iter; left time: 2997.2010s\n",
      "Epoch: 47 cost time: 56.25615096092224\n",
      "Epoch: 47, Steps: 286 | Train Loss: 0.4188207 Vali Loss: 0.4381332 Test Loss: 0.1747654\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Updating learning rate to 9.697737297875248e-07\n",
      "\titers: 100, epoch: 48 | loss: 0.7530866\n",
      "\tspeed: 0.4632s/iter; left time: 6974.9831s\n",
      "\titers: 200, epoch: 48 | loss: 0.2999845\n",
      "\tspeed: 0.1960s/iter; left time: 2932.6027s\n",
      "Epoch: 48 cost time: 56.093883991241455\n",
      "Epoch: 48, Steps: 286 | Train Loss: 0.4189733 Vali Loss: 0.4367707 Test Loss: 0.1740410\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Updating learning rate to 8.727963568087723e-07\n",
      "\titers: 100, epoch: 49 | loss: 0.3537310\n",
      "\tspeed: 0.4603s/iter; left time: 6800.3559s\n",
      "\titers: 200, epoch: 49 | loss: 0.3976589\n",
      "\tspeed: 0.1971s/iter; left time: 2891.6862s\n",
      "Epoch: 49 cost time: 56.3028404712677\n",
      "Epoch: 49, Steps: 286 | Train Loss: 0.4187534 Vali Loss: 0.4360514 Test Loss: 0.1744846\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Updating learning rate to 7.855167211278951e-07\n",
      "\titers: 100, epoch: 50 | loss: 0.3300861\n",
      "\tspeed: 0.4637s/iter; left time: 6717.5751s\n",
      "\titers: 200, epoch: 50 | loss: 0.3155998\n",
      "\tspeed: 0.1962s/iter; left time: 2822.7500s\n",
      "Epoch: 50 cost time: 56.29069137573242\n",
      "Epoch: 50, Steps: 286 | Train Loss: 0.4187646 Vali Loss: 0.4355575 Test Loss: 0.1745030\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Updating learning rate to 7.069650490151056e-07\n",
      "\titers: 100, epoch: 51 | loss: 0.3664862\n",
      "\tspeed: 0.4662s/iter; left time: 6620.6451s\n",
      "\titers: 200, epoch: 51 | loss: 0.2904210\n",
      "\tspeed: 0.1984s/iter; left time: 2797.3975s\n",
      "Epoch: 51 cost time: 56.58701515197754\n",
      "Epoch: 51, Steps: 286 | Train Loss: 0.4187588 Vali Loss: 0.4378797 Test Loss: 0.1744089\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Updating learning rate to 6.36268544113595e-07\n",
      "\titers: 100, epoch: 52 | loss: 0.3474309\n",
      "\tspeed: 0.4632s/iter; left time: 6445.8052s\n",
      "\titers: 200, epoch: 52 | loss: 0.3618365\n",
      "\tspeed: 0.1979s/iter; left time: 2734.2764s\n",
      "Epoch: 52 cost time: 56.382949113845825\n",
      "Epoch: 52, Steps: 286 | Train Loss: 0.4188921 Vali Loss: 0.4382164 Test Loss: 0.1746720\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Updating learning rate to 5.726416897022355e-07\n",
      "\titers: 100, epoch: 53 | loss: 0.3610711\n",
      "\tspeed: 0.4652s/iter; left time: 6340.0214s\n",
      "\titers: 200, epoch: 53 | loss: 0.5830544\n",
      "\tspeed: 0.1974s/iter; left time: 2670.3932s\n",
      "Epoch: 53 cost time: 56.30961227416992\n",
      "Epoch: 53, Steps: 286 | Train Loss: 0.4175183 Vali Loss: 0.4380205 Test Loss: 0.1745492\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Updating learning rate to 5.15377520732012e-07\n",
      "\titers: 100, epoch: 54 | loss: 0.3777761\n",
      "\tspeed: 0.4610s/iter; left time: 6150.6417s\n",
      "\titers: 200, epoch: 54 | loss: 0.3411483\n",
      "\tspeed: 0.1953s/iter; left time: 2586.5821s\n",
      "Epoch: 54 cost time: 55.931788206100464\n",
      "Epoch: 54, Steps: 286 | Train Loss: 0.4187836 Vali Loss: 0.4385507 Test Loss: 0.1745474\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Updating learning rate to 4.6383976865881085e-07\n",
      "\titers: 100, epoch: 55 | loss: 0.4201606\n",
      "\tspeed: 0.4605s/iter; left time: 6013.3362s\n",
      "\titers: 200, epoch: 55 | loss: 0.5508254\n",
      "\tspeed: 0.1950s/iter; left time: 2526.9400s\n",
      "Epoch: 55 cost time: 55.79655051231384\n",
      "Epoch: 55, Steps: 286 | Train Loss: 0.4175254 Vali Loss: 0.4379707 Test Loss: 0.1744915\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Updating learning rate to 4.174557917929298e-07\n",
      "\titers: 100, epoch: 56 | loss: 0.5402101\n",
      "\tspeed: 0.4611s/iter; left time: 5888.1655s\n",
      "\titers: 200, epoch: 56 | loss: 0.5249254\n",
      "\tspeed: 0.1951s/iter; left time: 2472.2250s\n",
      "Epoch: 56 cost time: 55.89964842796326\n",
      "Epoch: 56, Steps: 286 | Train Loss: 0.4172805 Vali Loss: 0.4389466 Test Loss: 0.1745394\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Updating learning rate to 3.7571021261363677e-07\n",
      "\titers: 100, epoch: 57 | loss: 0.2975053\n",
      "\tspeed: 0.4607s/iter; left time: 5751.2499s\n",
      "\titers: 200, epoch: 57 | loss: 0.4205091\n",
      "\tspeed: 0.1956s/iter; left time: 2422.3761s\n",
      "Epoch: 57 cost time: 55.96462273597717\n",
      "Epoch: 57, Steps: 286 | Train Loss: 0.4186702 Vali Loss: 0.4372496 Test Loss: 0.1743507\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Updating learning rate to 3.381391913522731e-07\n",
      "\titers: 100, epoch: 58 | loss: 0.7165745\n",
      "\tspeed: 0.4612s/iter; left time: 5625.9050s\n",
      "\titers: 200, epoch: 58 | loss: 0.3439304\n",
      "\tspeed: 0.1954s/iter; left time: 2364.2078s\n",
      "Epoch: 58 cost time: 55.901198863983154\n",
      "Epoch: 58, Steps: 286 | Train Loss: 0.4185769 Vali Loss: 0.4359807 Test Loss: 0.1745048\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Updating learning rate to 3.043252722170458e-07\n",
      "\titers: 100, epoch: 59 | loss: 0.3580447\n",
      "\tspeed: 0.4607s/iter; left time: 5487.9521s\n",
      "\titers: 200, epoch: 59 | loss: 0.3703540\n",
      "\tspeed: 0.1950s/iter; left time: 2303.9262s\n",
      "Epoch: 59 cost time: 55.79774737358093\n",
      "Epoch: 59, Steps: 286 | Train Loss: 0.4185869 Vali Loss: 0.4373212 Test Loss: 0.1743573\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Updating learning rate to 2.7389274499534124e-07\n",
      "\titers: 100, epoch: 60 | loss: 0.4671339\n",
      "\tspeed: 0.4608s/iter; left time: 5357.1640s\n",
      "\titers: 200, epoch: 60 | loss: 0.6709037\n",
      "\tspeed: 0.1958s/iter; left time: 2256.8307s\n",
      "Epoch: 60 cost time: 55.981268644332886\n",
      "Epoch: 60, Steps: 286 | Train Loss: 0.4183426 Vali Loss: 0.4386187 Test Loss: 0.1745013\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Updating learning rate to 2.465034704958071e-07\n",
      "\titers: 100, epoch: 61 | loss: 0.3553963\n",
      "\tspeed: 0.4610s/iter; left time: 5228.2666s\n",
      "\titers: 200, epoch: 61 | loss: 0.3098661\n",
      "\tspeed: 0.1946s/iter; left time: 2187.5820s\n",
      "Epoch: 61 cost time: 55.80740284919739\n",
      "Epoch: 61, Steps: 286 | Train Loss: 0.4186438 Vali Loss: 0.4360564 Test Loss: 0.1744734\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Updating learning rate to 2.218531234462264e-07\n",
      "\titers: 100, epoch: 62 | loss: 0.3694321\n",
      "\tspeed: 0.4607s/iter; left time: 5092.5758s\n",
      "\titers: 200, epoch: 62 | loss: 0.3547155\n",
      "\tspeed: 0.1949s/iter; left time: 2135.1431s\n",
      "Epoch: 62 cost time: 55.802942752838135\n",
      "Epoch: 62, Steps: 286 | Train Loss: 0.4185715 Vali Loss: 0.4374459 Test Loss: 0.1745341\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Updating learning rate to 1.9966781110160376e-07\n",
      "\titers: 100, epoch: 63 | loss: 0.4104868\n",
      "\tspeed: 0.4604s/iter; left time: 4957.7028s\n",
      "\titers: 200, epoch: 63 | loss: 0.3105930\n",
      "\tspeed: 0.1954s/iter; left time: 2084.7866s\n",
      "Epoch: 63 cost time: 55.872984170913696\n",
      "Epoch: 63, Steps: 286 | Train Loss: 0.4186974 Vali Loss: 0.4346446 Test Loss: 0.1744745\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Updating learning rate to 1.797010299914434e-07\n",
      "\titers: 100, epoch: 64 | loss: 0.6259430\n",
      "\tspeed: 0.4615s/iter; left time: 4838.3494s\n",
      "\titers: 200, epoch: 64 | loss: 0.2915340\n",
      "\tspeed: 0.1942s/iter; left time: 2016.4719s\n",
      "Epoch: 64 cost time: 55.67937660217285\n",
      "Epoch: 64, Steps: 286 | Train Loss: 0.4184331 Vali Loss: 0.4352168 Test Loss: 0.1743344\n",
      "EarlyStopping counter: 43 out of 100\n",
      "Updating learning rate to 1.6173092699229907e-07\n",
      "\titers: 100, epoch: 65 | loss: 0.3534478\n",
      "\tspeed: 0.4599s/iter; left time: 4689.2748s\n",
      "\titers: 200, epoch: 65 | loss: 0.4433862\n",
      "\tspeed: 0.1959s/iter; left time: 1977.5696s\n",
      "Epoch: 65 cost time: 55.99717092514038\n",
      "Epoch: 65, Steps: 286 | Train Loss: 0.4183130 Vali Loss: 0.4379390 Test Loss: 0.1744553\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Updating learning rate to 1.4555783429306916e-07\n",
      "\titers: 100, epoch: 66 | loss: 0.3916458\n",
      "\tspeed: 0.4617s/iter; left time: 4576.3925s\n",
      "\titers: 200, epoch: 66 | loss: 0.6413505\n",
      "\tspeed: 0.1954s/iter; left time: 1916.6259s\n",
      "Epoch: 66 cost time: 55.90266561508179\n",
      "Epoch: 66, Steps: 286 | Train Loss: 0.4182863 Vali Loss: 0.4367676 Test Loss: 0.1744178\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Updating learning rate to 1.3100205086376224e-07\n",
      "\titers: 100, epoch: 67 | loss: 0.6170056\n",
      "\tspeed: 0.4607s/iter; left time: 4434.0459s\n",
      "\titers: 200, epoch: 67 | loss: 0.3468663\n",
      "\tspeed: 0.1949s/iter; left time: 1856.7288s\n",
      "Epoch: 67 cost time: 55.84180665016174\n",
      "Epoch: 67, Steps: 286 | Train Loss: 0.4183347 Vali Loss: 0.4366686 Test Loss: 0.1743318\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Updating learning rate to 1.1790184577738603e-07\n",
      "\titers: 100, epoch: 68 | loss: 0.3375429\n",
      "\tspeed: 0.4611s/iter; left time: 4306.4387s\n",
      "\titers: 200, epoch: 68 | loss: 0.5571812\n",
      "\tspeed: 0.1952s/iter; left time: 1803.3241s\n",
      "Epoch: 68 cost time: 55.92069745063782\n",
      "Epoch: 68, Steps: 286 | Train Loss: 0.4185741 Vali Loss: 0.4371430 Test Loss: 0.1742900\n",
      "EarlyStopping counter: 47 out of 100\n",
      "Updating learning rate to 1.0611166119964742e-07\n",
      "\titers: 100, epoch: 69 | loss: 0.5583394\n",
      "\tspeed: 0.4618s/iter; left time: 4180.2686s\n",
      "\titers: 200, epoch: 69 | loss: 0.3712280\n",
      "\tspeed: 0.1950s/iter; left time: 1745.8532s\n",
      "Epoch: 69 cost time: 55.93575716018677\n",
      "Epoch: 69, Steps: 286 | Train Loss: 0.4174516 Vali Loss: 0.4363232 Test Loss: 0.1744009\n",
      "EarlyStopping counter: 48 out of 100\n",
      "Updating learning rate to 9.550049507968268e-08\n",
      "\titers: 100, epoch: 70 | loss: 0.2961714\n",
      "\tspeed: 0.4618s/iter; left time: 4048.9593s\n",
      "\titers: 200, epoch: 70 | loss: 0.3907267\n",
      "\tspeed: 0.1947s/iter; left time: 1687.4680s\n",
      "Epoch: 70 cost time: 55.8662474155426\n",
      "Epoch: 70, Steps: 286 | Train Loss: 0.4185320 Vali Loss: 0.4363373 Test Loss: 0.1745006\n",
      "EarlyStopping counter: 49 out of 100\n",
      "Updating learning rate to 8.595044557171442e-08\n",
      "\titers: 100, epoch: 71 | loss: 0.3552811\n",
      "\tspeed: 0.4612s/iter; left time: 3911.6697s\n",
      "\titers: 200, epoch: 71 | loss: 0.5460647\n",
      "\tspeed: 0.1950s/iter; left time: 1634.1808s\n",
      "Epoch: 71 cost time: 55.822938442230225\n",
      "Epoch: 71, Steps: 286 | Train Loss: 0.4185104 Vali Loss: 0.4391291 Test Loss: 0.1743321\n",
      "EarlyStopping counter: 50 out of 100\n",
      "Updating learning rate to 7.735540101454298e-08\n",
      "\titers: 100, epoch: 72 | loss: 0.3482537\n",
      "\tspeed: 0.4611s/iter; left time: 3778.3730s\n",
      "\titers: 200, epoch: 72 | loss: 0.4228561\n",
      "\tspeed: 0.1957s/iter; left time: 1583.9226s\n",
      "Epoch: 72 cost time: 56.02338981628418\n",
      "Epoch: 72, Steps: 286 | Train Loss: 0.4186542 Vali Loss: 0.4372526 Test Loss: 0.1744643\n",
      "EarlyStopping counter: 51 out of 100\n",
      "Updating learning rate to 6.961986091308869e-08\n",
      "\titers: 100, epoch: 73 | loss: 0.3200623\n",
      "\tspeed: 0.4621s/iter; left time: 3654.8445s\n",
      "\titers: 200, epoch: 73 | loss: 0.5682886\n",
      "\tspeed: 0.1955s/iter; left time: 1526.8883s\n",
      "Epoch: 73 cost time: 55.963244915008545\n",
      "Epoch: 73, Steps: 286 | Train Loss: 0.4171452 Vali Loss: 0.4391338 Test Loss: 0.1745095\n",
      "EarlyStopping counter: 52 out of 100\n",
      "Updating learning rate to 6.265787482177981e-08\n",
      "\titers: 100, epoch: 74 | loss: 0.5544320\n",
      "\tspeed: 0.4614s/iter; left time: 3516.9004s\n",
      "\titers: 200, epoch: 74 | loss: 0.5372889\n",
      "\tspeed: 0.1951s/iter; left time: 1468.0203s\n",
      "Epoch: 74 cost time: 55.86439609527588\n",
      "Epoch: 74, Steps: 286 | Train Loss: 0.4183474 Vali Loss: 0.4386763 Test Loss: 0.1743653\n",
      "EarlyStopping counter: 53 out of 100\n",
      "Updating learning rate to 5.639208733960184e-08\n",
      "\titers: 100, epoch: 75 | loss: 0.3167084\n",
      "\tspeed: 0.4619s/iter; left time: 3389.1352s\n",
      "\titers: 200, epoch: 75 | loss: 0.6444612\n",
      "\tspeed: 0.1946s/iter; left time: 1408.1970s\n",
      "Epoch: 75 cost time: 55.86461067199707\n",
      "Epoch: 75, Steps: 286 | Train Loss: 0.4184975 Vali Loss: 0.4393007 Test Loss: 0.1744466\n",
      "EarlyStopping counter: 54 out of 100\n",
      "Updating learning rate to 5.075287860564165e-08\n",
      "\titers: 100, epoch: 76 | loss: 0.3672887\n",
      "\tspeed: 0.4609s/iter; left time: 3249.5139s\n",
      "\titers: 200, epoch: 76 | loss: 0.3344051\n",
      "\tspeed: 0.1951s/iter; left time: 1356.2444s\n",
      "Epoch: 76 cost time: 55.8634352684021\n",
      "Epoch: 76, Steps: 286 | Train Loss: 0.4183866 Vali Loss: 0.4390419 Test Loss: 0.1744461\n",
      "EarlyStopping counter: 55 out of 100\n",
      "Updating learning rate to 4.567759074507749e-08\n",
      "\titers: 100, epoch: 77 | loss: 0.3267668\n",
      "\tspeed: 0.4604s/iter; left time: 3114.3670s\n",
      "\titers: 200, epoch: 77 | loss: 0.4285170\n",
      "\tspeed: 0.1953s/iter; left time: 1301.7174s\n",
      "Epoch: 77 cost time: 55.842957496643066\n",
      "Epoch: 77, Steps: 286 | Train Loss: 0.4183271 Vali Loss: 0.4391399 Test Loss: 0.1744931\n",
      "EarlyStopping counter: 56 out of 100\n",
      "Updating learning rate to 4.1109831670569744e-08\n",
      "\titers: 100, epoch: 78 | loss: 0.3589833\n",
      "\tspeed: 0.4605s/iter; left time: 2983.6927s\n",
      "\titers: 200, epoch: 78 | loss: 0.3882684\n",
      "\tspeed: 0.1947s/iter; left time: 1242.0738s\n",
      "Epoch: 78 cost time: 55.83131170272827\n",
      "Epoch: 78, Steps: 286 | Train Loss: 0.4184281 Vali Loss: 0.4376034 Test Loss: 0.1744580\n",
      "EarlyStopping counter: 57 out of 100\n",
      "Updating learning rate to 3.6998848503512764e-08\n",
      "\titers: 100, epoch: 79 | loss: 0.8196365\n",
      "\tspeed: 0.4610s/iter; left time: 2854.9507s\n",
      "\titers: 200, epoch: 79 | loss: 0.4650212\n",
      "\tspeed: 0.1953s/iter; left time: 1190.0808s\n",
      "Epoch: 79 cost time: 55.91613054275513\n",
      "Epoch: 79, Steps: 286 | Train Loss: 0.4186218 Vali Loss: 0.4370928 Test Loss: 0.1745028\n",
      "EarlyStopping counter: 58 out of 100\n",
      "Updating learning rate to 3.3298963653161496e-08\n",
      "\titers: 100, epoch: 80 | loss: 0.2988443\n",
      "\tspeed: 0.4618s/iter; left time: 2727.6469s\n",
      "\titers: 200, epoch: 80 | loss: 0.7670938\n",
      "\tspeed: 0.1955s/iter; left time: 1135.4099s\n",
      "Epoch: 80 cost time: 55.94244456291199\n",
      "Epoch: 80, Steps: 286 | Train Loss: 0.4183988 Vali Loss: 0.4365066 Test Loss: 0.1744300\n",
      "EarlyStopping counter: 59 out of 100\n",
      "Updating learning rate to 2.996906728784534e-08\n",
      "\titers: 100, epoch: 81 | loss: 0.3620713\n",
      "\tspeed: 0.4600s/iter; left time: 2585.4914s\n",
      "\titers: 200, epoch: 81 | loss: 0.3561038\n",
      "\tspeed: 0.1957s/iter; left time: 1080.3470s\n",
      "Epoch: 81 cost time: 56.16426610946655\n",
      "Epoch: 81, Steps: 286 | Train Loss: 0.4183480 Vali Loss: 0.4375412 Test Loss: 0.1743905\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Updating learning rate to 2.697216055906081e-08\n",
      "\titers: 100, epoch: 82 | loss: 0.3585738\n",
      "\tspeed: 0.4701s/iter; left time: 2507.9176s\n",
      "\titers: 200, epoch: 82 | loss: 0.2735690\n",
      "\tspeed: 0.1970s/iter; left time: 1031.2643s\n",
      "Epoch: 82 cost time: 56.64666032791138\n",
      "Epoch: 82, Steps: 286 | Train Loss: 0.4183993 Vali Loss: 0.4347085 Test Loss: 0.1744192\n",
      "EarlyStopping counter: 61 out of 100\n",
      "Updating learning rate to 2.427494450315473e-08\n",
      "\titers: 100, epoch: 83 | loss: 0.3810195\n",
      "\tspeed: 0.4628s/iter; left time: 2336.8023s\n",
      "\titers: 200, epoch: 83 | loss: 0.4423273\n",
      "\tspeed: 0.2006s/iter; left time: 992.6519s\n",
      "Epoch: 83 cost time: 57.17537975311279\n",
      "Epoch: 83, Steps: 286 | Train Loss: 0.4183920 Vali Loss: 0.4374079 Test Loss: 0.1744579\n",
      "EarlyStopping counter: 62 out of 100\n",
      "Updating learning rate to 2.1847450052839257e-08\n",
      "\titers: 100, epoch: 84 | loss: 0.5355963\n",
      "\tspeed: 0.4786s/iter; left time: 2279.8001s\n",
      "\titers: 200, epoch: 84 | loss: 0.3618142\n",
      "\tspeed: 0.1965s/iter; left time: 916.4154s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp(args)  \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[0;32m---> 21\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/01_projects/02_patchtst/PatchTST/PatchTST_supervised/exp/exp_main.py:135\u001b[0m, in \u001b[0;36mExp_Main.train\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    133\u001b[0m iter_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m model_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 135\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    138\u001b[0m batch_x_mark \u001b[38;5;241m=\u001b[39m batch_x_mark\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "                args.model_id,\n",
    "                args.model,\n",
    "                args.data,\n",
    "                args.features,\n",
    "                args.seq_len,\n",
    "                args.label_len,\n",
    "                args.pred_len,\n",
    "                args.d_model,\n",
    "                args.n_heads,\n",
    "                args.e_layers,\n",
    "                args.d_layers,\n",
    "                args.d_ff,\n",
    "                args.factor,\n",
    "                args.embed,\n",
    "                args.distil,\n",
    "                args.des, 1)\n",
    "\n",
    "exp = Exp(args)  # set experiments\n",
    "print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patchtst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
